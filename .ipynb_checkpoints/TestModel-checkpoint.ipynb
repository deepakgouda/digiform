{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.misc import toimage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3,padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*16,256)\n",
    "        self.fc2 = nn.Linear(256,47)\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 16*8*8)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[303 291', '17', '22 303 269', '17', '22 320 291', '19', '22 320 269', '19', '22 338 269\\n', '19', '22 339 291', '19', '21 357 291', '18', '21]']\n"
     ]
    }
   ],
   "source": [
    "f = open('Boxes.txt','r')\n",
    "lines = f.read().split('  ')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-57abaf491954>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-61-57abaf491954>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    image =\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#data = Image.open(\"data/d/54/hsf_0_00025.png\")\n",
    "image = \n",
    "\n",
    "for i in inputs:\n",
    "    for xt,yt,xb,yb in i[\"box\"]:\n",
    "        data = image[xt:xb,yt:yb,:]\n",
    "        data = data.resize((32,32))\n",
    "        in_data = np.transpose(data, (2,0,1))\n",
    "        final = in_data/255\n",
    "        images.append(final)\n",
    "        \n",
    "data = Variable(torch.Tensor(images))\n",
    "output = model(data)\n",
    "ans = torch.max(output,dim=1)[1]\n",
    "\n",
    "couter = 0\n",
    "for i in inputs:\n",
    "    for xt,yt,xb,yb in i[\"box\"]:\n",
    "        temp.append(ans[counter])\n",
    "        counter = counter + 1\n",
    "    temp2.append(temp)\n",
    "\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABoAAAAtCAIAAAA/V+yMAAAKHmlDQ1BJQ0MgUHJvZmlsZQAAeJy1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\nSdxh5v8BkCwGro/Nc+EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=26x45 at 0x2482E9DD438>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.open(\"trail.jpg\")\n",
    "t = im.crop((568,1010,594,1055))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =t.resize((32,32))\n",
    "in_data = np.transpose(t, (0,1,2))\n",
    "\n",
    "t = (in_data >150)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "toimage(t).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 8, 8])\n",
      "Variable containing:\n",
      " 37\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = Image.open(\"data/d/55_75/hsf_0_00008.png\")\n",
    "#data = data.resize((32,32))\n",
    "\n",
    "data = t\n",
    "in_data = np.transpose(data, (2,0,1))\n",
    "final = in_data/255\n",
    "images = []\n",
    "images.append(final)\n",
    "\n",
    "data = Variable(torch.Tensor(images))\n",
    "output = model(data)\n",
    "ans = torch.max(output,dim=1)[1]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yash = [[172, 371, 17, 22], [189, 371, 18, 22], [207, 371, 18, 22], [225, 371, 18, 22], [243, 371, 18, 22], [261, 371, 18, 22], [279, 371, 18, 22], [297, 371, 18, 22], [315, 371, 18, 22], [333, 371, 18, 22], [350, 371, 18, 22], [368, 371, 18, 22], [386, 371, 18, 22], [404, 371, 18, 22], [422, 371, 18, 22], [440, 371, 19, 22], [458, 370, 19, 23], [477, 370, 19, 23], [496, 370, 18, 23], [514, 370, 19, 23], [533, 370, 18, 23], [551, 370, 18, 23], [569, 370, 18, 23], [587, 370, 18, 23], [605, 370, 18, 23]]\n",
    "deepak = [[112, 21, 19, 25], [116, 27, 11, 16], [129, 21, 21, 25], [149, 23, 19, 23], [168, 22, 19, 24], [187, 22, 19, 23], [205, 22, 19, 23], [224, 22, 19, 23], [242, 22, 19, 23], [279, 22, 19, 23], [297, 21, 19, 23], [316, 21, 19, 23], [353, 21, 19, 23], [371, 21, 19, 22], [409, 21, 18, 22], [427, 21, 19, 22], [446, 21, 18, 22], [464, 21, 19, 22], [483, 21, 18, 22], [501, 21, 18, 22], [519, 21, 18, 22], [537, 21, 18, 22], [555, 21, 19, 22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"test3.jpg\")\n",
    "image = image.resize((588,650))\n",
    "\n",
    "images =[]\n",
    "for x,y,w,h in deepak:\n",
    "    data = image.crop((x,y,x+w,y+h))\n",
    "    data\n",
    "    data = data.resize((32,32))\n",
    "    in_data = np.transpose(data, (2,0,1))\n",
    "    in_data = (in_data >120)*255\n",
    "    final = in_data/255\n",
    "    images.append(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "data = Variable(torch.Tensor(images))\n",
    "output = model(data)\n",
    "ans = torch.max(output,dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,mapping = find_classes(\"data/d/\")\n",
    "mapping = {0: '30',\n",
    " 1: '31',\n",
    " 2: '32',\n",
    " 3: '33',\n",
    " 4: '34',\n",
    " 5: '35',\n",
    " 6: '36',\n",
    " 7: '37',\n",
    " 8: '38',\n",
    " 9: '39',\n",
    " 10: '41',\n",
    " 11: '42',\n",
    " 12: '43_63',\n",
    " 13: '44',\n",
    " 14: '45',\n",
    " 15: '46',\n",
    " 16: '47',\n",
    " 17: '48',\n",
    " 18: '49_69',\n",
    " 19: '4a_6a',\n",
    " 20: '4b_6b',\n",
    " 21: '4c_6c',\n",
    " 22: '4d_6d',\n",
    " 23: '4e',\n",
    " 24: '4f_6f',\n",
    " 25: '50_70',\n",
    " 26: '51',\n",
    " 27: '52',\n",
    " 28: '53_73',\n",
    " 29: '54',\n",
    " 30: '55_75',\n",
    " 31: '56_76',\n",
    " 32: '57_77',\n",
    " 33: '58_78',\n",
    " 34: '59_79',\n",
    " 35: '5a_7a',\n",
    " 36: '61',\n",
    " 37: '62',\n",
    " 38: '64',\n",
    " 39: '65',\n",
    " 40: '66',\n",
    " 41: '67',\n",
    " 42: '68',\n",
    " 43: '6e',\n",
    " 44: '71',\n",
    " 45: '72',\n",
    " 46: '74'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "d\n",
      "2\n",
      "B\n",
      "W\n",
      "L\n",
      "L\n",
      "U\n",
      "U\n",
      "2\n",
      "2\n",
      "1\n",
      "W\n",
      "G\n",
      "3\n",
      "1\n",
      "L\n",
      "1\n",
      "1\n",
      "L\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in ans:\n",
    "    temp = mapping[i.data.numpy()[0]][:2]\n",
    "    print(chr(int(temp,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_classes(dir):\n",
    "    classes = os.listdir(dir)\n",
    "    classes.sort()\n",
    "    class_to_idx = {i:classes[i] for i in range(len(classes))}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/d/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c43003c663f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/d/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-f7757ee09940>\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/d/'"
     ]
    }
   ],
   "source": [
    "_,mapping = find_classes(\"data/d/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deepak = [[112, 21, 19, 25], [116, 27, 11, 16], [129, 21, 21, 25], [149, 23, 19, 23], [168, 22, 19, 24], [187, 22, 19, 23], [205, 22, 19, 23], [224, 22, 19, 23], [242, 22, 19, 23], [279, 22, 19, 23], [297, 21, 19, 23], [316, 21, 19, 23], [353, 21, 19, 23], [371, 21, 19, 22], [409, 21, 18, 22], [427, 21, 19, 22], [446, 21, 18, 22], [464, 21, 19, 22], [483, 21, 18, 22], [501, 21, 18, 22], [519, 21, 18, 22], [537, 21, 18, 22], [555, 21, 19, 22]]\n",
    "x,y,w,h  = deepak[3]\n",
    "data = image.crop((x,y,x+w,y+h))\n",
    "data = data.resize((32,32))\n",
    "data\n",
    "\n",
    "in_data = np.transpose(data, (2,0,1))\n",
    "in_data = (in_data >120)*255\n",
    "in_data = np.transpose(in_data, (1,2,0))\n",
    "\n",
    "for i in range(len(in_data)):\n",
    "    for j in range(len(in_data[i])):\n",
    "        if not (in_data[i][j][0] == in_data[i][j][1] and in_data[i][j][1] == in_data[i][j][2]):\n",
    "            in_data[i][j][0] = 0\n",
    "            in_data[i][j][1] = 0\n",
    "            in_data[i][j][2] = 0\n",
    "        if i <= 3 or j <= 3 or i >= 29 or j >=29:\n",
    "            in_data[i][j][0] = 255\n",
    "            in_data[i][j][1] = 255\n",
    "            in_data[i][j][2] = 255\n",
    "in_data = np.transpose(in_data, (2,0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 8, 8])\n",
      "Variable containing:\n",
      " 10\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final = in_data/255\n",
    "images = []\n",
    "images.append(final)\n",
    "\n",
    "data1 = Variable(torch.Tensor(images))\n",
    "output = model(data1)\n",
    "ans = torch.max(output,dim=1)[1]\n",
    "print(ans)\n",
    "temp = mapping[ans.data.numpy()[0]][:2]\n",
    "print(chr(int(temp,16)))\n",
    "#print(plt.imshow(np.transpose(in_data, (1,2,0))))\n",
    "toimage(np.transpose(in_data, (1,2,0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '30',\n",
       " 1: '31',\n",
       " 2: '32',\n",
       " 3: '33',\n",
       " 4: '34',\n",
       " 5: '35',\n",
       " 6: '36',\n",
       " 7: '37',\n",
       " 8: '38',\n",
       " 9: '39',\n",
       " 10: '41',\n",
       " 11: '42',\n",
       " 12: '43_63',\n",
       " 13: '44',\n",
       " 14: '45',\n",
       " 15: '46',\n",
       " 16: '47',\n",
       " 17: '48',\n",
       " 18: '49_69',\n",
       " 19: '4a_6a',\n",
       " 20: '4b_6b',\n",
       " 21: '4c_6c',\n",
       " 22: '4d_6d',\n",
       " 23: '4e',\n",
       " 24: '4f_6f',\n",
       " 25: '50_70',\n",
       " 26: '51',\n",
       " 27: '52',\n",
       " 28: '53_73',\n",
       " 29: '54',\n",
       " 30: '55_75',\n",
       " 31: '56_76',\n",
       " 32: '57_77',\n",
       " 33: '58_78',\n",
       " 34: '59_79',\n",
       " 35: '5a_7a',\n",
       " 36: '61',\n",
       " 37: '62',\n",
       " 38: '64',\n",
       " 39: '65',\n",
       " 40: '66',\n",
       " 41: '67',\n",
       " 42: '68',\n",
       " 43: '6e',\n",
       " 44: '71',\n",
       " 45: '72',\n",
       " 46: '74'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
